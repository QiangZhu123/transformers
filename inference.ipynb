{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccff0ee",
   "metadata": {},
   "source": [
    "# bettertransfomrers \n",
    "    主要做融合和跳过算子的优化，同时，也把注意力变成flash attention\n",
    "    方法在PreTrainedModel中\n",
    "    optimum.BetterTransformer.transform\n",
    "    不支持8bit格式转换\n",
    "    遍历替换一些特定的模块层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14340624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cpu加速，这是把模型转化为更好实现的格式\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder\")\n",
    "model.to_bettertransformer()\n",
    "#model = model.reverse_bettertransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d406ad",
   "metadata": {},
   "source": [
    "# onnx 快速实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ef3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(\"optimum/roberta-base-squad2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "onnx_qa = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "question = \"What's my name?\"\n",
    "context = \"My name is Philipp and I live in Nuremberg.\"\n",
    "pred = onnx_qa(question, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#编译可以直接加速推断30%\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\").to(\"cuda\")\n",
    "model = torch.compile(model)#++++++++++++加速30%+++++++++++++\n",
    "\n",
    "processed_input = processor(image, return_tensors='pt').to(device=\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(**processed_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help函数，测量GPU内存占用峰值\n",
    "def bytes_to_giga_bytes(bytes):\n",
    "    return bytes / 1024 / 1024 / 1024\n",
    "bytes_to_giga_bytes(torch.cuda.max_memory_allocated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM模型高效执行，靠的是以下三个\n",
    "Lower Precision\n",
    "Flash Attention\n",
    "Architectural Innovations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lower Precision\n",
    "现在所有的LLM都是用bfloat16格式训练的from_pretrained(..., torch_dtype='bfloat16')\n",
    "\n",
    "量化过的模型这样用\n",
    "!pip install bitsandbytes\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/octocoder\", \n",
    "                                             load_in_8bit=True, \n",
    "                                             pad_token_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1870f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flash Attention1\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "    #pipe就是原来的模型制作的，只要这样用就能加快执行\n",
    "    #不用pipe也行，自己写model(**inputs)\n",
    "    result = pipe(long_prompt, max_new_tokens=60)[0][\"generated_text\"][len(long_prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c2998",
   "metadata": {},
   "outputs": [],
   "source": [
    "Architectural Innovations\n",
    "\n",
    "RoPE\n",
    "    \n",
    "key-value cache ：聊天任务就是靠着这个实现的\n",
    "generation_output = model.generate(**model_inputs, \n",
    "                                   max_new_tokens=60, \n",
    "                                   return_dict_in_generate=True,\n",
    "                                   do_sample=True#采样生成，结果多变\n",
    "                )\n",
    "decoded_output = tokenizer.batch_decode(generation_output.sequences)[0]\n",
    "\n",
    "prompt = decoded_output + \"\\nQuestion: How can I modify the function above to return Mega bytes instead?\\n\\nAnswer: Here\"\n",
    "model_inputs = tokenizer(prompt, return_tensors='pt')\n",
    "generation_output = model.generate(\n",
    "  **model_inputs,\n",
    "  past_key_values=generation_output.past_key_values,\n",
    "  max_new_tokens=60,\n",
    "  return_dict_in_generate=True\n",
    ")\n",
    "tokenizer.batch_decode(generation_output.sequences)[0][len(prompt):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a08f191",
   "metadata": {},
   "source": [
    "# 量化工作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee04a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q git+https://github.com/huggingface/transformers.git\n",
    "!pip install -U -q quanto\n",
    "!pip install -U -q accelerate\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8fff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"bigscience/bloom-560m\"\n",
    "device_map = {'model.embed_tokens': 0,\n",
    " 'model.layers.0': 0,\n",
    " 'model.layers.1': 0,\n",
    " 'model.layers.2': 0,\n",
    " 'model.layers.26': 0,\n",
    " 'model.layers.27': 'cpu',\n",
    " 'model.layers.28': 'cpu',\n",
    " 'model.layers.31': 'disk',\n",
    " 'model.norm': 'cpu',\n",
    " 'lm_head':0}\n",
    "quantization_config = QuantoConfig(weights=\"int8\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             torch_dtype=torch.float32,\n",
    "                                             quantization_config=quantization_config,\n",
    "                                             device_map=device_map)\n",
    "\n",
    "#把参数传入就完成量化\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3920d76a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello my name is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"Hello my name is\"\n",
    "device = \"cuda\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ad8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a5e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
