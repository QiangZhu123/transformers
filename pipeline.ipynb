{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据给定的参数，使用AutoConfig,AutoModelForpretrain,等映射机制，实例化流程组件\n",
    "#task,model参数任意选择\n",
    "#返回AutoPipeline的\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c57267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task='name',model='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ae517d",
   "metadata": {},
   "source": [
    "# 基类PIPELINE\n",
    "    就是推断模型的类，对于不同的任务都要实现一个特定的子类，因为不同任务预处理和后处理方法不同\n",
    "    子类需要实现以下三个方法\n",
    "    preprocess：预处理输入[str,PIL.image]\n",
    "        inPut处理，一个样本，载入或读取\n",
    "        需要调用image_processor，\n",
    "        最后返回字典，要生成对应的键值对{'image':input}\n",
    "    _forward\n",
    "        从上面的结果取出输入\n",
    "        调用model(**input)\n",
    "        返回模型特定输出类\n",
    "    postprocess：\n",
    "        后处理模型输出类，\n",
    "        可能会再次调用image_processor.post_process函数\n",
    "        解码生成annotation 字典\n",
    "    使用pipeline，会调用__call__方法，所以是直接当作模型使用即可  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbacacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#也可以将pipeline拆开使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b283b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "\n",
    "checkpoint = \"vinvino02/glpn-nyu\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = AutoModelForDepthEstimation.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2af6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values)\n",
    "    predicted_depth = outputs.predicted_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# interpolate to original size\n",
    "prediction = torch.nn.functional.interpolate(\n",
    "    predicted_depth.unsqueeze(1),\n",
    "    size=image.size[::-1],\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,\n",
    ").squeeze()\n",
    "output = prediction.numpy()\n",
    "\n",
    "formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n",
    "depth = Image.fromarray(formatted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
