{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11152a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Method/tool                        Improves training speed             Optimizes memory utilization\n",
    "Batch size choice                           Yes                                   Yes\n",
    "Gradient accumulation  #å‡é€Ÿ                No                                    Yes\n",
    "Gradient checkpointing#å‡é€Ÿ20%              No                                    Yes\n",
    "Mixed precision training #fp16=True,        Yes                                   No\n",
    "Optimizer choice                            Yes                                   Yes\n",
    "Data preloading                             Yes                                    No\n",
    "DeepSpeed Zero                              No                                    Yes\n",
    "torch.compile    è¿™ä¸ªæœ‰è®¾å¤‡è¦,æé€Ÿ30%       Yes                                    No\n",
    "Parameter-Efficient Fine Tuning (PEFT)      No                                     Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc1426",
   "metadata": {},
   "source": [
    "# æ•°æ®ï¼Œéš¾ç‚¹å°±åœ¨äºæ•°æ®çš„å¤„ç†\n",
    "# å’Œå¦‚ä½•å»è¿›è¡Œä»»åŠ¡è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision.transforms import ColorJitter,RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "from transformers import AutoImageProcessor\n",
    "dataset = load_dataset(\"scene_parse_150\",\n",
    "                  #split=\"train[:50]\",é€‰æ‹©trainä¸­çš„å‰50ä¸ªæ ·æœ¬\n",
    "                 )\n",
    "dataset= dataset.train_test_split(test_size=0.2)#éœ€è¦å¯¹æ•´ä½“æ•°æ®é›†è¿›è¡Œåˆ‡åˆ†\n",
    "train_data = dataset['train']\n",
    "test_data=dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a0695",
   "metadata": {},
   "outputs": [],
   "source": [
    "classname = train_data.features['label'].names\n",
    "id2label = {int(k): v for k, v in enumerate(classname)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a90bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¢ç›Šå›¾ç‰‡\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n",
    "# ä¼ å…¥çš„å°±æ˜¯åˆ—è¡¨ï¼Œè¿™ä¸ªè°ƒæ•´å›¾ç‰‡å¤§å°ï¼Œè¿˜æœ‰æ ‡å‡†åŒ–ï¼Œä¸åšå¢ç›Š\n",
    "checkpoint = \"nvidia/mit-b0\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=True)\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30380eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¼ å…¥ä¸€ä¸ªæ•´åˆçš„batchï¼Œæ‰€ä»¥è¦éå†æ¯ä¸€ä¸ªæ ·æœ¬è¿›è¡Œå¢ç›Š\n",
    "#è€Œå¢ç›Šåˆæ˜¯ è‡ªå®šä¹‰å‡½æ•°+image_processorçš„å½¢å¼ï¼Œæ‰€ä»¥è¿™é‡Œæ˜¯è®©æ•°æ®æ ¼å¼æ»¡è¶³img_processorå¯ä»¥ä½¿ç”¨çš„å½¢å¼\n",
    "#è¿™é‡Œè¿˜æ˜¯datasetï¼Œä¸æ˜¯dataloader\n",
    "\n",
    "#è¿™æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæ¯ä¸ªvalueå·²ç»è¢«æ•´æ•´åˆäº†\n",
    "#æ‰€ä»¥æœ€å¥½è¿›è¡Œéå†çš„æ–¹å¼å¤„ç†\n",
    "def train_transforms(example_batch):\n",
    "    #example_batch  ={â€˜image':[],'label':[]}\n",
    "    example_batch['pixel_values']=[_transforms(x.convert(\"RGB\")) for x in example_batch['image']]\n",
    "    del example_batch['image']\n",
    "    return example_batch\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    example_batch['pixel_values']=[_transforms(x.convert(\"RGB\")) for x in example_batch['image']]\n",
    "    del example_batch['image']\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å‡½æ•°å¿…é¡»æ¥å—ä¸€ä¸ªå­—å…¸ï¼Œå­—å…¸çš„å¥å°±æ˜¯ä¸€ä¸ªæ ·æœ¬ä¸­çš„ä¸€æ ·ï¼Œåªæ˜¯å®ƒçš„å€¼æ˜¯åˆ—è¡¨ï¼Œéœ€è¦éå†æ¯ä¸€ä¸ªè¿›è¡Œå¢ç›Š\n",
    "#\n",
    "#data = train_ds.map(func)è¿™ç§å½¢å¼ï¼Œfuncå¿…é¡»æ˜¯å¤„ç†ä¸€ä¸ªæ ·æœ¬çš„ï¼Œè¿”å›ä¸€ä¸ªå­—å…¸\n",
    "# function (`callable`): with one of the following signature:\n",
    "#    - `function(example: Dict) -> Union[Dict, Any]` if `batched=False` and `with_indices=False`\n",
    "#    - `function(example: Dict, indices: int) -> Union[Dict, Any]` if `batched=False` and `with_indices=True`\n",
    "#    - `function(batch: Dict[List]) -> Union[Dict, Any]` if `batched=True` and `with_indices=False`\n",
    "#    - `function(batch: Dict[List], indices: List[int]) -> Union[Dict, Any]` if `batched=True` and `with_indices=True`\n",
    "\n",
    "#. The transform is applied on-the-fly on batches\n",
    "train_ds=train_ds.with_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2be6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#è¿™ä¸ªæ˜¯å¸¸è§„çš„collate_fnå‡½æ•°ï¼Œå°±æ˜¯å°†è®¸å¤šä¸ªå­—å…¸ï¼Œæ•´åˆæˆä¸€ä¸ªå­—å…¸\n",
    "#å¯ä»¥ç›´æ¥å½“ä½œDataLoaderä¸­çš„collate_fnå‡½æ•°ç›´æ¥ä¼ å…¥\n",
    "#ç”Ÿæˆç»™æ¨¡å‹ä½¿ç”¨çš„ä¸€ä¸ªå­—å…¸ï¼Œè¦ä¿è¯é”®å€¼æ­£ç¡®\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "from transformers import DefaultDataCollator\n",
    "#å°±æ˜¯æŠŠæ•°æ®æåˆæˆå¼ é‡ï¼Œè¿™ä¸ªæ˜¯éœ€è¦çš„\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab26900a",
   "metadata": {},
   "source": [
    "# è¯„ä¼°æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56751ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pycocotools\n",
    "#!pip install evaluate\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "EvalPrediction(predictions=preds, label_ids=label_ids, inputs=inputs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb3e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "è¯„ä¼°æ•°æ®é›†ä¸º {\"pixel_values\": pixel_values, \"labels\": {\"image_id\": image_id, \"annotations\": target}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8845945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    #eval_pred æ˜¯ä¸€ä¸ªç»“æœbatchï¼Œevalæ¨¡å¼ä¸­ï¼ŒæŠŠæ¯ä¸ªbatchç»“æœæ·»åŠ åˆ°metricä¸­\n",
    "    #è¿™æ˜¯ä¸€ä¸ªä¸­ä»‹å‡½æ•°ï¼ŒæŠŠçœŸæ­£çš„è®¡ç®—äº¤ç»™metric\n",
    "    #è¿™å°±æ˜¯è¿™ä¸ªæ¡†æ¶çš„ç¼ºé™·ï¼Œè¾“å…¥å°±æ˜¯æ¨¡å‹è¾“å‡ºï¼Œè¦è¯„ä¼°éœ€è¦åœ¨è¿™é‡Œåšåå¤„ç†æ‰è¡Œ\n",
    "    with torch.no_grad():\n",
    "        logits_tensor,labels=eval_pred\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65869bfc",
   "metadata": {},
   "source": [
    "# æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23983f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n",
    "#modelçš„å®šä¹‰è¦æ±‚  ï¼š forwardè¾“å‡ºä¸€ä¸ªå¤§ç±»ï¼Œå¿…é¡»åŒæ—¶åŒ…å«losså’Œprediction\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint,\n",
    "                                                         id2label=id2label, \n",
    "                                                         label2id=label2id,\n",
    "                                                        ignore_mismatched_sizes=True).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¼˜åŒ–å™¨é€‰æ‹©\n",
    "vars(transformers.training_args.OptimizerNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb900d3d",
   "metadata": {},
   "source": [
    "# è®­ç»ƒç›¸å…³çš„æ‰€æœ‰å‚æ•°ï¼Œæ²¡æœ‰æ¨¡å‹å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7485e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "args= TrainingArguments('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537f46cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression expected after dictionary key and ':' (4032607840.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    'evaluation_strategy': <IntervalStrategy.NO: 'no'>,#è¯„ä¼°æ˜¯æŒ‰epochè¿˜æ˜¯iter,æˆ–è€…ä¸è¯„ä¼°[no,step,epoch]\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expression expected after dictionary key and ':'\n"
     ]
    }
   ],
   "source": [
    "{'output_dir': '',\n",
    "  'learning_rate': 5e-05,#åˆå§‹åŒ–å­¦ä¹ ç‡\n",
    "  'per_device_train_batch_size': 8,#batchå¤§å°,8æˆ–64çš„å€æ•°\n",
    "  'num_train_epochs': 3.0,#è®­ç»ƒæ¬¡æ•°\n",
    " 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,#å­¦ä¹ ç‡å˜åŒ– \n",
    "  'optim': <OptimizerNames.ADAMW_HF: 'adamw_hf'>,#ä¼˜åŒ–å™¨ adamw_hf, adamw_torch, adamw_torch_fused, \n",
    "                                              # adamw_apex_fused, adamw_anyprecision oradafactor.\n",
    "                                             #å¦‚æœä¸æŒ‡å®šï¼Œè¦åœ¨trainerä¸­è¿›è¡Œå®ä¾‹åŒ–\n",
    " 'optim_args': None,#ä¼˜åŒ–å™¨å‚æ•°\n",
    " 'adafactor': False, #æ˜¯å¦æŠŠadamwæ›¿æ¢ä¸ºadafacotor\n",
    " #è¿™äº›æ˜¯adamWè®­ç»ƒå™¨çš„å‚æ•°\n",
    " 'weight_decay': 0.0,\n",
    " 'adam_beta1': 0.9,\n",
    " 'adam_beta2': 0.999,\n",
    " 'adam_epsilon': 1e-08,\n",
    " 'max_grad_norm': 1.0, \n",
    " 'warmup_ratio': 0.0,#å‚æ•°\n",
    " 'warmup_steps': 0, \n",
    " \n",
    " 'bf16': False, #æ˜¯å¦ç”¨bf16è®­ç»ƒæ¨¡å‹\n",
    " 'fp16': False,#æ˜¯å¦æ··åˆç²¾åº¦è®­ç»ƒæ¨¡å‹ \n",
    "  'fp16_opt_level': 'O1',#æ··åˆç²¾åº¦apexä¸­çš„è®¾ç½®å‚æ•°\n",
    " 'half_precision_backend': 'auto',# \"auto\", \"cuda_amp\", \"apex\", \"cpu_amp\" åç«¯é€‰æ‹©ï¼Œä¸€èˆ¬ç”¨autoå°±æ€§äº†\n",
    " 'bf16_full_eval': False,#bf16è¯„ä¼°\n",
    " 'fp16_full_eval': False,#fp16è¯„ä¼°\n",
    " 'gradient_accumulation_steps': 1,#ç´¯åŠ å¤šå°‘æ­¥æ¢¯åº¦æ‰æ›´æ–°å‚æ•°\n",
    "   'gradient_checkpointing': False,#ä¿å­˜æ¢¯åº¦ç”¨äºæ›´æ–°å‚æ•°\n",
    " \n",
    "  'dataloader_drop_last': False,#æœ€åä¸€ä¸ªä¸è¶³batchæ˜¯å¦ä¸¢æ‰\n",
    " 'dataloader_num_workers': 0,#éœ€è¦å¤šå°‘è¿›ç¨‹\n",
    " 'label_smoothing_factor': 0.0,#æ ‡ç­¾å¹³æ»‘çš„ç³»æ•°ï¼Œ0å°±æ˜¯ä¸ç”¨ ã€0ï¼Œ1ã€‘\n",
    " \n",
    " 'evaluation_strategy': <IntervalStrategy.NO: 'no'>,#è¯„ä¼°æ˜¯æŒ‰epochè¿˜æ˜¯iter,æˆ–è€…ä¸è¯„ä¼°[no,steps,epoch]\n",
    " 'per_device_eval_batch_size': 8,\n",
    "\n",
    " 'eval_accumulation_steps': None,#é¢„æµ‹ç”Ÿæˆå¤šå°‘æ­¥æ‰æŠŠç»“æœæ¬è¿åˆ°CPUä¸Š\n",
    " 'eval_delay': 0,  #ç¬¬ä¸€æ­¥è¯„ä¼°å¼€å§‹å‰ï¼Œæ‰§è¡Œè®­ç»ƒå¤šå°‘æ¬¡ï¼Œæ¨è¿Ÿè¯„ä¼°çš„ä½œç”¨\n",
    " \n",
    "  'logging_steps': 500,#è®°å½•\n",
    "  'logging_dir': 'runs\\\\Dec02_22-12-45_DESKTOP-RJGFSP1',#æ—¥å¿—ä¿å­˜è·¯å¾„\n",
    " \n",
    " 'save_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
    " 'save_steps': 500,#ä¿å­˜æ­¥æ•°\n",
    " 'save_total_limit': None,\n",
    " 'save_safetensors': False,\n",
    " 'save_on_each_node': False,\n",
    "\n",
    " 'seed': 42,\n",
    " 'data_seed': None,#æ•°æ®é‡‡æ ·çš„ç§å­\n",
    "  'dataloader_pin_memory': True,#pin_memoryè®¾ç½®\n",
    " 'resume_from_checkpoint': None,#ç»§ç»­è®­ç»ƒï¼Œä¼ å…¥æ–‡ä»¶\n",
    " \n",
    " \n",
    " 'jit_mode_eval': False,#ç”¨jitæ ¼å¼çš„æ¨¡å‹è¿›è¡Œæ¨æ–­\n",
    " 'use_ipex': False,#Intel extension ä¼˜åŒ–pytorchçš„æ–¹æ³•ï¼Œæ˜¯å¦ä½¿ç”¨\n",
    " \n",
    " 'tf32': None,#tf32æ ¼å¼\n",
    " 'local_rank': 0,#åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°\n",
    " 'ddp_backend': None,#\"nccl\"`, `\"mpi\"`, `\"ccl\"`, `\"gloo\"åç«¯é€‰æ‹©\n",
    " 'sharded_ddp': [],#ç”¨Sharded DDP training å‚æ•°ä¸ºsimple\"ï¼Œzero_dp_2ï¼Œzero_dp_3ï¼Œoffload\n",
    " 'fsdp': [],#pytorchåˆ†å¸ƒå¼è®­ç»ƒ  ï¼Œfull_shardï¼Œshard_grad_opï¼Œoffloadï¼Œauto_wrap\n",
    "  'ddp_find_unused_parameters': None,#åˆ†å¸ƒå¼è®­ç»ƒä¸­find_unused_parameterså‚æ•°\n",
    " 'ddp_bucket_cap_mb': None,#åˆ†å¸ƒå¼è®­ç»ƒä¸­find_unused_parameterså‚æ•°\n",
    "  'ddp_timeout': 1800,\n",
    " 'fsdp_config': {'fsdp_min_num_params': 0,\n",
    "  'xla': False,\n",
    "  'xla_fsdp_grad_ckpt': False},#è¿™é‡Œä¹Ÿæœ‰è®¸å¤šå‚æ•°å¯ä»¥ä½¿ç”¨\n",
    " \n",
    " 'deepspeed': None,#æ˜¯å¦ä½¿ç”¨deepspeed\n",
    " 'report_to': ['tensorboard', 'wandb'],#åç«¯çš„æ”¯æŒæœ‰å¾ˆå¤šï¼Œ'all'\n",
    " \n",
    " 'include_inputs_for_metrics': False,#æ˜¯å¦æŠŠè¾“å…¥ä¼ é€’ç»™merics\n",
    "\n",
    "\n",
    " 'torch_compile': True,#ç¼–è¯‘æ¨¡å‹,å¦‚æœç‰ˆæœ¬ä¸å¯¹,ç”¨torch._dynamo.config.suppress_errors = True\n",
    " 'overwrite_output_dir': False,#æ˜¯å¦ä¿®æ”¹ä¿å­˜è·¯å¾„\n",
    "'max_steps': -1,#ä¼šè¦†ç›–num_train_epochs çš„è®­ç»ƒæ¬¡æ•°ï¼Œä¸€æ ·çš„\n",
    " \n",
    " #æ—¥å¿—ç›¸å…³å‚æ•°\n",
    "  'log_level': 'passive',\n",
    " 'log_level_replica': 'warning',\n",
    " 'log_on_each_node': True,\n",
    " 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
    " 'logging_first_step': False,\n",
    "  'logging_nan_inf_filter': True,\n",
    "  'tpu_num_cores': None,#å¦‚æœåœ¨TPUä¸Šè®­ç»ƒï¼Œéœ€è¦è®¾ç½®æ ¸å¿ƒä¸ªæ•°\n",
    " 'tpu_metrics_debug': False,\n",
    " 'debug': [],\n",
    "  'eval_steps': None,#\n",
    "  'past_index': -1,#ä¸€äº›æ¨¡å‹ç”¨äºè¾“å‡ºçš„å±‚\n",
    "  'run_name': '',#wandbè¦ç”¨çš„\n",
    "  'disable_tqdm': False,#æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹\n",
    "  'remove_unused_columns': True,#æ˜¯å¦å»æ‰?\n",
    "  'label_names': None,\n",
    " \n",
    "  'load_best_model_at_end': False,\n",
    "  'metric_for_best_model': None,\n",
    "  'greater_is_better': None,\n",
    " \n",
    " 'ignore_data_skip': False,#ç»§ç»­è®­ç»ƒä¼šå¿«ç‚¹\n",
    "  'group_by_length': False,#\n",
    "  'length_column_name': 'length',\n",
    "  'skip_memory_metrics': True,#ä¸€èˆ¬ä¸ºTureï¼Œä¸ç„¶ä¼šå‡æ…¢è®­ç»ƒå’Œè¯„ä¼°é€Ÿåº¦\n",
    "  'use_legacy_prediction_loop': False,\n",
    "  'push_to_hub': False, # æ¶å¿ƒçš„è®¾ç½®\n",
    "  'hub_model_id': None,\n",
    " 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,\n",
    " 'hub_token': None,\n",
    " 'hub_private_repo': False,\n",
    " 'prediction_loss_only': False,\n",
    " 'push_to_hub_model_id': None,\n",
    " 'push_to_hub_organization': None,\n",
    " 'push_to_hub_token': None,\n",
    "  'mp_parameters': '',#sagmaker\n",
    "  'full_determinism': False,#debugæ‰ä¼šç”¨è¿™ä¸ª\n",
    "  'ray_scope': 'last',#æœç´¢è¶…å‚æ•°\n",
    "  'do_train': False,  #  æ˜¯å¦è®­ç»ƒ\n",
    " 'do_eval': False,  #    æ˜¯å¦è¯„ä¼°   è¿™ä¸ªä¸‰ä¸ªå‚æ•°ä¼šè‡ªåŠ¨è®¾ç½®\n",
    " 'do_predict': False,#æ˜¯å¦é¢„æµ‹\n",
    "  'auto_find_batch_size': False,#å½“CUDAå†…å­˜ä¸å¤Ÿï¼ŒæŠŠbatchå¤§å°å‡å°‘ä¸€åŠ\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a346ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¼˜åŒ–å™¨é€‰æ‹©\n",
    "\"adamw_hf\"ï¼Œ\"adamw_torch\"ï¼Œ\"adamw_torch_fused\"ï¼Œ\n",
    "\"adamw_torch_xla\"ï¼Œ\"adamw_apex_fused\"ï¼Œ\n",
    "\"adafactor\"ï¼Œ\"adamw_anyprecision\"ï¼Œ \"sgd\"ï¼Œ\"adagrad\"ï¼Œ\n",
    "\"adamw_bnb_8bit\"ï¼Œ\"adamw_8bit\"  ï¼Œ\"lion_8bit\"ï¼Œ\n",
    "\"lion_32bit\"ï¼Œ\"paged_adamw_32bit\"ï¼Œ \"paged_adamw_8bit\"ï¼Œ\n",
    "\"paged_lion_32bit\"ï¼Œ \"paged_lion_8bit\"ï¼Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfdb680",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_food_model\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    per_device_train_batch_size=64,\n",
    "    \n",
    "    warmup_ratio=0.1,\n",
    "    optimizers= (optimizer,None),  # optimizer=torch.optim.SGD(model.parameters(),lr=1e-5)\n",
    "    \n",
    "    gradient_accumulation_steps=1,\n",
    "    \n",
    "    #evaluateï¼Œå¯ä»¥ç›´æ¥å»æ‰\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    eval_steps=5,\n",
    "    \n",
    "    fp16=True,\n",
    "    optim='adafactor',\n",
    "    dataloader_pin_memory =True,\n",
    "    dataloader_num_workers=2,\n",
    "    \n",
    "    logging_steps=10,\n",
    "    \n",
    "    save_steps=5,\n",
    "    report_to=\"tensorboard\",\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    remove_unused_columns=False,#è¿™ä¸ªæ˜¯å…ˆåˆ é™¤æ²¡ç”¨çš„å†åå¤„ç†ï¼Œä»¥ä¸€å®šè¦æ³¨æ„ï¼Œåªèƒ½å»åˆ é™¤å“ªäº›ä¸€å®šä¸ä¼šç”¨çš„\n",
    "    \n",
    "    label_names=[\"labels\"],#è¿™ä¸ªå¿…é¡»ç»™ï¼Œä¸ç„¶å®¹æ˜“æŠ¥é”™label,label_idéƒ½è¢«å ç”¨äº†ï¼Œlabelsç‰¹æŒ‡åˆ†ç±»æ ‡ç­¾ï¼Œä¼šç”¨label_smoothæ¥å¤„ç†\n",
    "\n",
    "    auto_find_batch_size=False,#å½“CUDAå†…å­˜ä¸å¤Ÿï¼ŒæŠŠbatchå¤§å°å‡å°‘ä¸€åŠ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a578b",
   "metadata": {},
   "source": [
    "# deepspeed\n",
    "    git clone https://github.com/microsoft/DeepSpeedExamples\n",
    "    cd DeepSpeedExamples\n",
    "    find . -name '*json'\n",
    "    # find examples with the Lamb optimizer\n",
    "    grep -i Lamb $(find . -name '*json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c9f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingArguments(..., deepspeed=\"path/to/deepspeed_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f45533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63997f42",
   "metadata": {},
   "source": [
    "# è®­ç»ƒå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7942ea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword argument repeated: per_device_train_batch_size (3791893107.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    per_device_train_batch_size=1,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword argument repeated: per_device_train_batch_size\n"
     ]
    }
   ],
   "source": [
    "#å¯ä»¥ä½¿ç”¨apex\n",
    "trainer = Trainer(\n",
    "    model=model,#ç›´æ¥æ˜¯æ¨¡å‹å°±å¯ä»¥\n",
    "    model_init= None,#å’Œmodelä¸¤ä¸ªå‚æ•°å¿…é¡»ç»™ä¸€ä¸ª\n",
    "    args=training_args,# å‚æ•°ï¼Œå¯ä»¥æ˜¯ç©ºï¼Œé»˜è®¤æœ‰\n",
    "    data_collator=data_collator,#å¯ä»¥è®¾ä¸ºç©ºï¼Œè¿™æ˜¯æœ‰é»˜è®¤default_data_collatorå¤„ç†å­—å…¸æ•°æ®çš„\n",
    "    train_dataset=mnist[\"train\"],\n",
    "    tokenizer=image_processor,#ç”¨äºä¿å­˜çš„ï¼Œ\n",
    "    \n",
    "    #eval_dataset=mnist[\"test\"],\n",
    "    #compute_metrics=compute_metrics,#è¯„ä¼°å‡½æ•°\n",
    "    tokenizer= None,#æ–‡æœ¬å¯èƒ½ä¼šç”¨åˆ°\n",
    "    callbacks= None,#é’©å­ï¼Œæœ‰é»˜è®¤çš„ï¼Œå¦‚æœç»™äº†å°±æ˜¯å’Œé»˜è®¤çš„ç»„åˆåœ¨ä¸€èµ·æ‰€æœ‰çš„\n",
    "    optimizers= (None, None),#ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡\n",
    "    preprocess_logits_for_metrics= None,\n",
    ")\n",
    "\n",
    "trainer.train(\n",
    "    #resume_from_checkpoint='/kaggle/working/detr-resnet-50_finetuned_cppe5/checkpoint-200/'\n",
    ")#è®­ç»ƒæ˜¯è®­ç»ƒï¼Œè¯„ä¼°æ˜¯è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainerä¸­çš„compute_loss(self, model, inputs, return_outputs=False)å°±æ˜¯ç”¨\n",
    "\n",
    "    model(**inputs)çš„æ–¹æ³•è®¡ç®—æŸå¤±{'loss':value}çš„ï¼Œæ‰€ä»¥æ¨¡å‹æ‰§è¡Œå¿…é¡»æ˜¯èƒ½è®¡ç®—æŸå¤±çš„\n",
    "    åœ¨è¿™è°ƒç”¨losså‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b2aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(resume_from_checkpoint='/kaggle/working/detr-resnet-50_finetuned_cppe5/checkpoint-200/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6efba0",
   "metadata": {},
   "source": [
    "# LORAè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e34b216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig,get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç”¨æ³•,æ‰€æœ‰é‡è¦å‚æ•°ä¿å­˜ï¼ŒLoRA\n",
    "from peft import LoraConfig, TaskType,get_peft_model\n",
    "\n",
    "#################è‡ªå®šä¹‰æ¨¡å‹ä¸­ä½¿ç”¨peft######################\n",
    "#1å®šä¹‰æ¨¡å‹ï¼Œæ˜¯ä¸€ä¸ªnn.moduleå°±è¡Œäº†\n",
    "class Mymodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        return \n",
    "#æ ¹æ®æ¯ä¸€å±‚çš„åå­—ï¼Œæ¥ç¡®å®šå…·ä½“å¯¹\n",
    "print([(n, type(m)) for n, m in Mymodel().named_modules()])\n",
    "peft_config = LoraConfig(target_modules=[\"layer1\", \"layer2\"],#æƒ³åˆ†è§£æ›¿æ¢çš„ï¼Œä¸èƒ½æ˜¯æœ€åä¸€å±‚ï¼Œä¸ç„¶è¾“å‡ºå˜äº†\n",
    "                         modules_to_save=[\"layer3\"],#æƒ³ä¾ç„¶è®­ç»ƒçš„,ä¿ç•™æœ€åä¸€å±‚\n",
    "                         r=8, \n",
    "                         lora_alpha=32,\n",
    "                         lora_dropout=0.1)\n",
    "\n",
    "#only `torch.nn.Linear` and `Conv1D` are supported.\n",
    "model = MLP()\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "# prints trainable params: 56,164 || all params: 4,100,164 || trainable%: 1.369798866581922\n",
    "\n",
    "#è®­ç»ƒå‚æ•°ä¸­ï¼Œå¯ä»¥ä½¿ç”¨æ›´å¤§çš„Batchï¼Œå’Œæ›´å¤§çš„å­¦ä¹ ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73bb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#æ–°çš„æ–¹å¼æ˜¯è¿™æ ·ç”¨çš„\n",
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    modules_to_save=[\"lm_head\"],\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db49963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    '''\n",
    "    ç»§æ‰¿trainerï¼Œéœ€è¦ä¿®æ”¹é‡Œé¢çš„ compute_loss(self, model, inputs, return_outputs=False)\n",
    "    è¿”å›ä¸€ä¸ªlossæ ‡é‡\n",
    "    '''\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ccc551",
   "metadata": {},
   "source": [
    "# ä¼˜åŒ–è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079040da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using ğŸ¤— PEFT\n",
    "\n",
    "#ä½¿ç”¨acclerator\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)\n",
    "\n",
    "if training_args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "accelerator = Accelerator(fp16=training_args.fp16)\n",
    "model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)\n",
    "\n",
    "model.train()\n",
    "for step, batch in enumerate(dataloader, start=1):\n",
    "    loss = model(**batch).loss\n",
    "    loss = loss / training_args.gradient_accumulation_steps\n",
    "    accelerator.backward(loss)\n",
    "    if step % training_args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "#ç”¨å¤šä¸“å®¶æ¨¡å‹\n",
    "\n",
    "#torch.nn.functional.scaled_dot_product_attention (SDPA)\n",
    "#FLashattention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0efa41",
   "metadata": {},
   "source": [
    "# å¤šGPU è®­ç»ƒä¼˜åŒ–\n",
    "    \n",
    "    \n",
    "    1æ¨¡å‹å¯ä»¥æ”¾å…¥å•ä¸ªGPUæ—¶\n",
    "        å¯ä»¥ä½¿ç”¨DDPæˆ–è€…ZERO\n",
    "    2å½“æ¨¡å‹ä¸èƒ½æ”¾å…¥å•ä¸ªGPUæ—¶ï¼Œå¤ªå¤§äº†\n",
    "        ç”¨PP,ZERO,TP\n",
    "    3æ¨¡å‹æœ€å¤§çš„å±‚æ”¾ä¸è¿›ä¸€ä¸ªGPU\n",
    "        TP\n",
    "        \n",
    "     DPå’ŒDPPçš„åŒºåˆ«ï¼ŒDPæ˜¯ç”¨GPU0æ¥æ‰§è¡Œå½’ä¸€åŒ–å’Œä»»åŠ¡åˆ†å‘ \n",
    "     1DPPæ¯ä¸ªbatchåªé€šä¿¡ä¸€æ¬¡ï¼ŒDDPæœ‰5ä¸ªé€šä¿¡ï¼Œè€Œä¸”DPç”¨GIL\n",
    "     2DPä¸­GPU0æœ‰å¤§é‡å·¥ä½œè¦åš\n",
    "     3DDPæ”¯æŒè·¨æœºå™¨çš„åˆ†å¸ƒå¼è®­ç»ƒï¼ŒDPä¸è¡Œ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814233aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20eab3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698d1cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7cf5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724021db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a501330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667526a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65cfdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
