{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11152a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Method/tool                        Improves training speed             Optimizes memory utilization\n",
    "Batch size choice                           Yes                                   Yes\n",
    "Gradient accumulation  #减速                No                                    Yes\n",
    "Gradient checkpointing#减速20%              No                                    Yes\n",
    "Mixed precision training #fp16=True,        Yes                                   No\n",
    "Optimizer choice                            Yes                                   Yes\n",
    "Data preloading                             Yes                                    No\n",
    "DeepSpeed Zero                              No                                    Yes\n",
    "torch.compile    这个有设备要,提速30%       Yes                                    No\n",
    "Parameter-Efficient Fine Tuning (PEFT)      No                                     Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc1426",
   "metadata": {},
   "source": [
    "# 数据，难点就在于数据的处理\n",
    "# 和如何去进行任务评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision.transforms import ColorJitter,RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "from transformers import AutoImageProcessor\n",
    "dataset = load_dataset(\"scene_parse_150\",\n",
    "                  #split=\"train[:50]\",选择train中的前50个样本\n",
    "                 )\n",
    "dataset= dataset.train_test_split(test_size=0.2)#需要对整体数据集进行切分\n",
    "train_data = dataset['train']\n",
    "test_data=dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a0695",
   "metadata": {},
   "outputs": [],
   "source": [
    "classname = train_data.features['label'].names\n",
    "id2label = {int(k): v for k, v in enumerate(classname)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0a90bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#增益图片\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n",
    "# 传入的就是列表，这个调整图片大小，还有标准化，不做增益\n",
    "checkpoint = \"nvidia/mit-b0\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=True)\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30380eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#传入一个整合的batch，所以要遍历每一个样本进行增益\n",
    "#而增益又是 自定义函数+image_processor的形式，所以这里是让数据格式满足img_processor可以使用的形式\n",
    "#这里还是dataset，不是dataloader\n",
    "\n",
    "#这是一个字典，每个value已经被整整合了\n",
    "#所以最好进行遍历的方式处理\n",
    "def train_transforms(example_batch):\n",
    "    #example_batch  ={‘image':[],'label':[]}\n",
    "    example_batch['pixel_values']=[_transforms(x.convert(\"RGB\")) for x in example_batch['image']]\n",
    "    del example_batch['image']\n",
    "    return example_batch\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    example_batch['pixel_values']=[_transforms(x.convert(\"RGB\")) for x in example_batch['image']]\n",
    "    del example_batch['image']\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cc43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数必须接受一个字典，字典的健就是一个样本中的一样，只是它的值是列表，需要遍历每一个进行增益\n",
    "#\n",
    "#data = train_ds.map(func)这种形式，func必须是处理一个样本的，返回一个字典\n",
    "# function (`callable`): with one of the following signature:\n",
    "#    - `function(example: Dict) -> Union[Dict, Any]` if `batched=False` and `with_indices=False`\n",
    "#    - `function(example: Dict, indices: int) -> Union[Dict, Any]` if `batched=False` and `with_indices=True`\n",
    "#    - `function(batch: Dict[List]) -> Union[Dict, Any]` if `batched=True` and `with_indices=False`\n",
    "#    - `function(batch: Dict[List], indices: List[int]) -> Union[Dict, Any]` if `batched=True` and `with_indices=True`\n",
    "\n",
    "#. The transform is applied on-the-fly on batches\n",
    "train_ds=train_ds.with_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2be6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这个是常规的collate_fn函数，就是将许多个字典，整合成一个字典\n",
    "#可以直接当作DataLoader中的collate_fn函数直接传入\n",
    "#生成给模型使用的一个字典，要保证键值正确\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "from transformers import DefaultDataCollator\n",
    "#就是把数据捏合成张量，这个是需要的\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab26900a",
   "metadata": {},
   "source": [
    "# 评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56751ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pycocotools\n",
    "#!pip install evaluate\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "EvalPrediction(predictions=preds, label_ids=label_ids, inputs=inputs_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb3e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "评估数据集为 {\"pixel_values\": pixel_values, \"labels\": {\"image_id\": image_id, \"annotations\": target}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8845945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    #eval_pred 是一个结果batch，eval模式中，把每个batch结果添加到metric中\n",
    "    #这是一个中介函数，把真正的计算交给metric\n",
    "    #这就是这个框架的缺陷，输入就是模型输出，要评估需要在这里做后处理才行\n",
    "    with torch.no_grad():\n",
    "        logits_tensor,labels=eval_pred\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65869bfc",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23983f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n",
    "#model的定义要求  ： forward输出一个大类，必须同时包含loss和prediction\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint,\n",
    "                                                         id2label=id2label, \n",
    "                                                         label2id=label2id,\n",
    "                                                        ignore_mismatched_sizes=True).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#优化器选择\n",
    "vars(transformers.training_args.OptimizerNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb900d3d",
   "metadata": {},
   "source": [
    "# 训练相关的所有参数，没有模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7485e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "args= TrainingArguments('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537f46cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression expected after dictionary key and ':' (4032607840.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    'evaluation_strategy': <IntervalStrategy.NO: 'no'>,#评估是按epoch还是iter,或者不评估[no,step,epoch]\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expression expected after dictionary key and ':'\n"
     ]
    }
   ],
   "source": [
    "{'output_dir': '',\n",
    "  'learning_rate': 5e-05,#初始化学习率\n",
    "  'per_device_train_batch_size': 8,#batch大小,8或64的倍数\n",
    "  'num_train_epochs': 3.0,#训练次数\n",
    " 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>,#学习率变化 \n",
    "  'optim': <OptimizerNames.ADAMW_HF: 'adamw_hf'>,#优化器 adamw_hf, adamw_torch, adamw_torch_fused, \n",
    "                                              # adamw_apex_fused, adamw_anyprecision oradafactor.\n",
    "                                             #如果不指定，要在trainer中进行实例化\n",
    " 'optim_args': None,#优化器参数\n",
    " 'adafactor': False, #是否把adamw替换为adafacotor\n",
    " #这些是adamW训练器的参数\n",
    " 'weight_decay': 0.0,\n",
    " 'adam_beta1': 0.9,\n",
    " 'adam_beta2': 0.999,\n",
    " 'adam_epsilon': 1e-08,\n",
    " 'max_grad_norm': 1.0, \n",
    " 'warmup_ratio': 0.0,#参数\n",
    " 'warmup_steps': 0, \n",
    " \n",
    " 'bf16': False, #是否用bf16训练模型\n",
    " 'fp16': False,#是否混合精度训练模型 \n",
    "  'fp16_opt_level': 'O1',#混合精度apex中的设置参数\n",
    " 'half_precision_backend': 'auto',# \"auto\", \"cuda_amp\", \"apex\", \"cpu_amp\" 后端选择，一般用auto就性了\n",
    " 'bf16_full_eval': False,#bf16评估\n",
    " 'fp16_full_eval': False,#fp16评估\n",
    " 'gradient_accumulation_steps': 1,#累加多少步梯度才更新参数\n",
    "   'gradient_checkpointing': False,#保存梯度用于更新参数\n",
    " \n",
    "  'dataloader_drop_last': False,#最后一个不足batch是否丢掉\n",
    " 'dataloader_num_workers': 0,#需要多少进程\n",
    " 'label_smoothing_factor': 0.0,#标签平滑的系数，0就是不用 【0，1】\n",
    " \n",
    " 'evaluation_strategy': <IntervalStrategy.NO: 'no'>,#评估是按epoch还是iter,或者不评估[no,steps,epoch]\n",
    " 'per_device_eval_batch_size': 8,\n",
    "\n",
    " 'eval_accumulation_steps': None,#预测生成多少步才把结果搬运到CPU上\n",
    " 'eval_delay': 0,  #第一步评估开始前，执行训练多少次，推迟评估的作用\n",
    " \n",
    "  'logging_steps': 500,#记录\n",
    "  'logging_dir': 'runs\\\\Dec02_22-12-45_DESKTOP-RJGFSP1',#日志保存路径\n",
    " \n",
    " 'save_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
    " 'save_steps': 500,#保存步数\n",
    " 'save_total_limit': None,\n",
    " 'save_safetensors': False,\n",
    " 'save_on_each_node': False,\n",
    "\n",
    " 'seed': 42,\n",
    " 'data_seed': None,#数据采样的种子\n",
    "  'dataloader_pin_memory': True,#pin_memory设置\n",
    " 'resume_from_checkpoint': None,#继续训练，传入文件\n",
    " \n",
    " \n",
    " 'jit_mode_eval': False,#用jit格式的模型进行推断\n",
    " 'use_ipex': False,#Intel extension 优化pytorch的方法，是否使用\n",
    " \n",
    " 'tf32': None,#tf32格式\n",
    " 'local_rank': 0,#分布式训练参数\n",
    " 'ddp_backend': None,#\"nccl\"`, `\"mpi\"`, `\"ccl\"`, `\"gloo\"后端选择\n",
    " 'sharded_ddp': [],#用Sharded DDP training 参数为simple\"，zero_dp_2，zero_dp_3，offload\n",
    " 'fsdp': [],#pytorch分布式训练  ，full_shard，shard_grad_op，offload，auto_wrap\n",
    "  'ddp_find_unused_parameters': None,#分布式训练中find_unused_parameters参数\n",
    " 'ddp_bucket_cap_mb': None,#分布式训练中find_unused_parameters参数\n",
    "  'ddp_timeout': 1800,\n",
    " 'fsdp_config': {'fsdp_min_num_params': 0,\n",
    "  'xla': False,\n",
    "  'xla_fsdp_grad_ckpt': False},#这里也有许多参数可以使用\n",
    " \n",
    " 'deepspeed': None,#是否使用deepspeed\n",
    " 'report_to': ['tensorboard', 'wandb'],#后端的支持有很多，'all'\n",
    " \n",
    " 'include_inputs_for_metrics': False,#是否把输入传递给merics\n",
    "\n",
    "\n",
    " 'torch_compile': True,#编译模型,如果版本不对,用torch._dynamo.config.suppress_errors = True\n",
    " 'overwrite_output_dir': False,#是否修改保存路径\n",
    "'max_steps': -1,#会覆盖num_train_epochs 的训练次数，一样的\n",
    " \n",
    " #日志相关参数\n",
    "  'log_level': 'passive',\n",
    " 'log_level_replica': 'warning',\n",
    " 'log_on_each_node': True,\n",
    " 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>,\n",
    " 'logging_first_step': False,\n",
    "  'logging_nan_inf_filter': True,\n",
    "  'tpu_num_cores': None,#如果在TPU上训练，需要设置核心个数\n",
    " 'tpu_metrics_debug': False,\n",
    " 'debug': [],\n",
    "  'eval_steps': None,#\n",
    "  'past_index': -1,#一些模型用于输出的层\n",
    "  'run_name': '',#wandb要用的\n",
    "  'disable_tqdm': False,#显示训练过程\n",
    "  'remove_unused_columns': True,#是否去掉?\n",
    "  'label_names': None,\n",
    " \n",
    "  'load_best_model_at_end': False,\n",
    "  'metric_for_best_model': None,\n",
    "  'greater_is_better': None,\n",
    " \n",
    " 'ignore_data_skip': False,#继续训练会快点\n",
    "  'group_by_length': False,#\n",
    "  'length_column_name': 'length',\n",
    "  'skip_memory_metrics': True,#一般为Ture，不然会减慢训练和评估速度\n",
    "  'use_legacy_prediction_loop': False,\n",
    "  'push_to_hub': False, # 恶心的设置\n",
    "  'hub_model_id': None,\n",
    " 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>,\n",
    " 'hub_token': None,\n",
    " 'hub_private_repo': False,\n",
    " 'prediction_loss_only': False,\n",
    " 'push_to_hub_model_id': None,\n",
    " 'push_to_hub_organization': None,\n",
    " 'push_to_hub_token': None,\n",
    "  'mp_parameters': '',#sagmaker\n",
    "  'full_determinism': False,#debug才会用这个\n",
    "  'ray_scope': 'last',#搜索超参数\n",
    "  'do_train': False,  #  是否训练\n",
    " 'do_eval': False,  #    是否评估   这个三个参数会自动设置\n",
    " 'do_predict': False,#是否预测\n",
    "  'auto_find_batch_size': False,#当CUDA内存不够，把batch大小减少一半\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a346ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#优化器选择\n",
    "\"adamw_hf\"，\"adamw_torch\"，\"adamw_torch_fused\"，\n",
    "\"adamw_torch_xla\"，\"adamw_apex_fused\"，\n",
    "\"adafactor\"，\"adamw_anyprecision\"， \"sgd\"，\"adagrad\"，\n",
    "\"adamw_bnb_8bit\"，\"adamw_8bit\"  ，\"lion_8bit\"，\n",
    "\"lion_32bit\"，\"paged_adamw_32bit\"， \"paged_adamw_8bit\"，\n",
    "\"paged_lion_32bit\"， \"paged_lion_8bit\"，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfdb680",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_food_model\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    \n",
    "    per_device_train_batch_size=64,\n",
    "    \n",
    "    warmup_ratio=0.1,\n",
    "    optimizers= (optimizer,None),  # optimizer=torch.optim.SGD(model.parameters(),lr=1e-5)\n",
    "    \n",
    "    gradient_accumulation_steps=1,\n",
    "    \n",
    "    #evaluate，可以直接去掉\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    eval_steps=5,\n",
    "    \n",
    "    fp16=True,\n",
    "    optim='adafactor',\n",
    "    dataloader_pin_memory =True,\n",
    "    dataloader_num_workers=2,\n",
    "    \n",
    "    logging_steps=10,\n",
    "    \n",
    "    save_steps=5,\n",
    "    report_to=\"tensorboard\",\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    remove_unused_columns=False,#这个是先删除没用的再后处理，以一定要注意，只能去删除哪些一定不会用的\n",
    "    \n",
    "    label_names=[\"labels\"],#这个必须给，不然容易报错label,label_id都被占用了，labels特指分类标签，会用label_smooth来处理\n",
    "\n",
    "    auto_find_batch_size=False,#当CUDA内存不够，把batch大小减少一半\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435a578b",
   "metadata": {},
   "source": [
    "# deepspeed\n",
    "    git clone https://github.com/microsoft/DeepSpeedExamples\n",
    "    cd DeepSpeedExamples\n",
    "    find . -name '*json'\n",
    "    # find examples with the Lamb optimizer\n",
    "    grep -i Lamb $(find . -name '*json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c9f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingArguments(..., deepspeed=\"path/to/deepspeed_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f45533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63997f42",
   "metadata": {},
   "source": [
    "# 训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7942ea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword argument repeated: per_device_train_batch_size (3791893107.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    per_device_train_batch_size=1,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword argument repeated: per_device_train_batch_size\n"
     ]
    }
   ],
   "source": [
    "#可以使用apex\n",
    "trainer = Trainer(\n",
    "    model=model,#直接是模型就可以\n",
    "    model_init= None,#和model两个参数必须给一个\n",
    "    args=training_args,# 参数，可以是空，默认有\n",
    "    data_collator=data_collator,#可以设为空，这是有默认default_data_collator处理字典数据的\n",
    "    train_dataset=mnist[\"train\"],\n",
    "    tokenizer=image_processor,#用于保存的，\n",
    "    \n",
    "    #eval_dataset=mnist[\"test\"],\n",
    "    #compute_metrics=compute_metrics,#评估函数\n",
    "    tokenizer= None,#文本可能会用到\n",
    "    callbacks= None,#钩子，有默认的，如果给了就是和默认的组合在一起所有的\n",
    "    optimizers= (None, None),#优化器和学习率\n",
    "    preprocess_logits_for_metrics= None,\n",
    ")\n",
    "\n",
    "trainer.train(\n",
    "    #resume_from_checkpoint='/kaggle/working/detr-resnet-50_finetuned_cppe5/checkpoint-200/'\n",
    ")#训练是训练，评估是评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer中的compute_loss(self, model, inputs, return_outputs=False)就是用\n",
    "\n",
    "    model(**inputs)的方法计算损失{'loss':value}的，所以模型执行必须是能计算损失的\n",
    "    在这调用loss函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b2aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(resume_from_checkpoint='/kaggle/working/detr-resnet-50_finetuned_cppe5/checkpoint-200/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6efba0",
   "metadata": {},
   "source": [
    "# LORA训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e34b216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig,get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f71d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#用法,所有重要参数保存，LoRA\n",
    "from peft import LoraConfig, TaskType,get_peft_model\n",
    "\n",
    "#################自定义模型中使用peft######################\n",
    "#1定义模型，是一个nn.module就行了\n",
    "class Mymodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        return \n",
    "#根据每一层的名字，来确定具体对\n",
    "print([(n, type(m)) for n, m in Mymodel().named_modules()])\n",
    "peft_config = LoraConfig(target_modules=[\"layer1\", \"layer2\"],#想分解替换的，不能是最后一层，不然输出变了\n",
    "                         modules_to_save=[\"layer3\"],#想依然训练的,保留最后一层\n",
    "                         r=8, \n",
    "                         lora_alpha=32,\n",
    "                         lora_dropout=0.1)\n",
    "\n",
    "#only `torch.nn.Linear` and `Conv1D` are supported.\n",
    "model = MLP()\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "# prints trainable params: 56,164 || all params: 4,100,164 || trainable%: 1.369798866581922\n",
    "\n",
    "#训练参数中，可以使用更大的Batch，和更大的学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73bb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#新的方式是这样用的\n",
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    modules_to_save=[\"lm_head\"],\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db49963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    '''\n",
    "    继承trainer，需要修改里面的 compute_loss(self, model, inputs, return_outputs=False)\n",
    "    返回一个loss标量\n",
    "    '''\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0], device=model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ccc551",
   "metadata": {},
   "source": [
    "# 优化训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079040da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using 🤗 PEFT\n",
    "\n",
    "#使用acclerator\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)\n",
    "\n",
    "if training_args.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "accelerator = Accelerator(fp16=training_args.fp16)\n",
    "model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)\n",
    "\n",
    "model.train()\n",
    "for step, batch in enumerate(dataloader, start=1):\n",
    "    loss = model(**batch).loss\n",
    "    loss = loss / training_args.gradient_accumulation_steps\n",
    "    accelerator.backward(loss)\n",
    "    if step % training_args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "#用多专家模型\n",
    "\n",
    "#torch.nn.functional.scaled_dot_product_attention (SDPA)\n",
    "#FLashattention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0efa41",
   "metadata": {},
   "source": [
    "# 多GPU 训练优化\n",
    "    \n",
    "    \n",
    "    1模型可以放入单个GPU时\n",
    "        可以使用DDP或者ZERO\n",
    "    2当模型不能放入单个GPU时，太大了\n",
    "        用PP,ZERO,TP\n",
    "    3模型最大的层放不进一个GPU\n",
    "        TP\n",
    "        \n",
    "     DP和DPP的区别，DP是用GPU0来执行归一化和任务分发 \n",
    "     1DPP每个batch只通信一次，DDP有5个通信，而且DP用GIL\n",
    "     2DP中GPU0有大量工作要做\n",
    "     3DDP支持跨机器的分布式训练，DP不行\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814233aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20eab3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698d1cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7cf5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724021db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a501330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667526a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65cfdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
